{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUXrRDJFtM9Q"
      },
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/google/gemma-7b-it\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/gemma-7b-it)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPUcja_D2INL",
        "outputId": "d5270b59-0b15-48cd-a28b-13f0ad77ef17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m133.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4500226 sha256=50e7406bc26401f92e7956715fdc48ce45d46dbe510df1e1ff10f946fd7e990a\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, pandas, llama-cpp-python\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.16 pandas-2.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -U llama-cpp-python pandas openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE2zpCGitM9R"
      },
      "source": [
        "\\The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ü§ó"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "4248a72d602f472c82de00a0713cd2c5",
            "cf2623cb5b194ca79d1c38240c45730b",
            "83d1f71ca5ff415cb6f5d10dfbb9849e",
            "37c30623fb5a47559bdf98322212a10b",
            "897942a248204ba1b5e367c5a0ec554f",
            "f3694105712340c19324c32473eba8f6",
            "3a6f79ba35e543f98c04d3ef7abae456",
            "5a7768b778e24c78871e4ce10c34d8ee",
            "4a2be3cfc9b14a3a9303d7591bc2a14f",
            "c8af8f6d8adf4118a101667633e53c2a",
            "3fe4f3ac082c4465b4553d036df9d65d",
            "6852f56b1dea4e94b2642011f160aa1e",
            "9f80f5bee44e4b119faf1af4751f672d",
            "afda049be6b54038a56c4f5ec80c219d",
            "4b7ee0e1d36443388429bb451dd27315",
            "e99d569b9d034e20a916ead54af5d111",
            "d0e9900f465649bbaebb14ba195f108c",
            "5cf18c71d015404b902431e9fb8f829f",
            "5289fc786365486791ee1bb0897a678c",
            "54b4aa6716b74cb693ebe29d817695ac"
          ]
        },
        "id": "VWdXKlX3tM9R",
        "outputId": "35db9f16-81e0-4e38-8157-7bd826db9055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4248a72d602f472c82de00a0713cd2c5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LOokMmstM9R"
      },
      "outputs": [],
      "source": [
        "# !pip install llama-cpp-python\n",
        "import pandas as pd\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "L0rGnBZG348B",
        "outputId": "88f9c21e-148d-46af-8397-7a07d3305adf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b18530a6-1072-4db8-af83-4f3af6d4de4e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b18530a6-1072-4db8-af83-4f3af6d4de4e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PV_Signal_List_10_TEST_SCENARIOS.xlsx to PV_Signal_List_10_TEST_SCENARIOS.xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-RLvZxq4X4c",
        "outputId": "1f76f77e-5b63-44ed-f4d4-587223fcba71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Signal term Date detected                                     Status  \\\n",
            "0           Stroke        May-25                                    Ongoing   \n",
            "1              SJS        Jun-25                                    Refuted   \n",
            "2      Myocarditis        Apr-25   Closed as Potential Risk (Not Important)   \n",
            "3  QT prolongation        Sep-24       Closed as Potential Risk (Important)   \n",
            "4     Pancreatitis        Mar-25  Closed as Identified Risk (Not Important)   \n",
            "\n",
            "  Date closed     Source of signal  \\\n",
            "0         NaN        Meta-analysis   \n",
            "1      25-Aug         Case reports   \n",
            "2      10-Jul             Registry   \n",
            "3      12-Oct       Clinical Study   \n",
            "4      09-Sep  Spontaneous Reports   \n",
            "\n",
            "                                          Summary              Action  \n",
            "0                   Signal still under assessment             Pending  \n",
            "1          Evaluation shows no causal association                 NaN  \n",
            "2  Evidence suggests possible risk but low impact             Monitor  \n",
            "3       Signal indicates important potential risk        Label update  \n",
            "4     Identified risk confirmed but not important  Routine monitoring  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"PV_Signal_List_10_TEST_SCENARIOS.xlsx\")\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3d5b607265764a2a979f1ba9beadf3c9",
            "2d285cd704b141608b7054a0c651d4ea",
            "48209765da0842248e1a087ddea7d7bd",
            "f5f5e7133e564d2da7c9a29c4bad50bf",
            "cdcdc9ebf26b4b7689579cf6bd355bce",
            "7a2e256d2ecd4bd0bd106975a958e2d5",
            "88f6b93f8fc24e97b7c92ad1344b0cc8",
            "4785113e0e3b4334a0732a1cfe68502c",
            "49fe9956ea784c89ac8e6cf3fedbe3f3",
            "f59396ed2db64d0c98e5705197d1f35e",
            "af7046445f3040b3874f073444262151"
          ]
        },
        "id": "8x8HvtZJ3hLg",
        "outputId": "aca3883f-c958-4007-9935-f11eba19bf89"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "./gemma-7b-it.gguf:   0%|          | 0.00/34.2G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d5b607265764a2a979f1ba9beadf3c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 254 tensors from /root/.cache/huggingface/hub/models--google--gemma-7b-it/snapshots/9c5798d27f588501ce1e108079d2a19e4c3a2353/./gemma-7b-it.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
            "llama_model_loader: - kv   1:                               general.name str              = gemma-7b-it\n",
            "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                          gemma.block_count u32              = 28\n",
            "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
            "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
            "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - type  f32:  254 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = all F32 (guessed)\n",
            "print_info: file size   = 31.81 GiB (32.00 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control-looking token:    107 '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control token:      2 '<bos>' is not marked as EOG\n",
            "load: control token:      0 '<pad>' is not marked as EOG\n",
            "load: control token:      1 '<eos>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 1 ('<eos>')\n",
            "load:   - 107 ('<end_of_turn>')\n",
            "load: special tokens cache size = 5\n",
            "load: token to piece cache size = 1.6014 MB\n",
            "print_info: arch             = gemma\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 8192\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 16\n",
            "print_info: n_head_kv        = 16\n",
            "print_info: n_rot            = 256\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 256\n",
            "print_info: n_embd_head_v    = 256\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 4096\n",
            "print_info: n_embd_v_gqa     = 4096\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 24576\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 8192\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 8.54 B\n",
            "print_info: general.name     = gemma-7b-it\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 256128\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 2 '<bos>'\n",
            "print_info: EOS token        = 1 '<eos>'\n",
            "print_info: EOT token        = 107 '<end_of_turn>'\n",
            "print_info: UNK token        = 3 '<unk>'\n",
            "print_info: PAD token        = 0 '<pad>'\n",
            "print_info: LF token         = 227 '<0x0A>'\n",
            "print_info: EOG token        = 1 '<eos>'\n",
            "print_info: EOG token        = 107 '<end_of_turn>'\n",
            "print_info: max token length = 93\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (f32) (and 254 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size = 32570.17 MiB\n",
            "............................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.98 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =  1792.00 MiB\n",
            "llama_kv_cache_unified: size = 1792.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  896.00 MiB, V (f16):  896.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 2032\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   512.25 MiB\n",
            "llama_context: graph nodes  = 1015\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '24576', 'gemma.attention.head_count': '16', 'general.name': 'gemma-7b-it', 'gemma.context_length': '8192', 'gemma.block_count': '28', 'gemma.embedding_length': '3072', 'gemma.attention.head_count_kv': '16', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"google/gemma-7b-it\",\n",
        "    filename=\"gemma-7b-it.gguf\",\n",
        "    n_ctx=4096,    # context length\n",
        "    n_gpu_layers=-1,  # use GPU if available\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bXG37Dw33SC"
      },
      "outputs": [],
      "source": [
        "# Define the system instruction (same as your LLaMA setup)\n",
        "system_prompt = (\n",
        "    \"You are an expert in pharmacovigilance and ICH E2C(R2) guidelines. \"\n",
        "    \"Your task is to read ONLY the input safety signal data exactly as provided \"\n",
        "    \"and map each signal to the correct PBRER section(s) according to Appendix F. \"\n",
        "    \"Do NOT infer medical judgment beyond what is explicitly stated.\\n\\n\"\n",
        "\n",
        "    \"Use ONLY the following seven scenarios when categorizing signals:\\n\"\n",
        "    \"1. Ongoing Signal\\n\"\n",
        "    \"2. Closed + Refuted\\n\"\n",
        "    \"3. Closed ‚Üí Potential Risk (NOT important)\\n\"\n",
        "    \"4. Closed ‚Üí Potential Risk (IMPORTANT)\\n\"\n",
        "    \"5. Closed ‚Üí Identified Risk (NOT important)\\n\"\n",
        "    \"6. Closed ‚Üí Identified Risk (IMPORTANT)\\n\"\n",
        "    \"7. Closed ‚Üí New Information on Known Risk\\n\\n\"\n",
        "\n",
        "    \"Mapping logic:\\n\"\n",
        "    \"- Ongoing Signal ‚Üí Section 15 only.\\n\"\n",
        "    \"- Closed + Refuted ‚Üí Section 15 + Section 16.2.\\n\"\n",
        "    \"- Closed ‚Üí Potential Risk (NOT important) ‚Üí Section 15 + Section 16.2.\\n\"\n",
        "    \"- Closed ‚Üí Potential Risk (IMPORTANT) ‚Üí Section 15 + Section 16.2 + Section 16.4.\\n\"\n",
        "    \"- Closed ‚Üí Identified Risk (NOT important) ‚Üí Section 15 + Section 16.2.\\n\"\n",
        "    \"- Closed ‚Üí Identified Risk (IMPORTANT) ‚Üí Section 15 + Section 16.2 + Section 16.4.\\n\"\n",
        "    \"- Closed ‚Üí New Information on Known Risk ‚Üí Section 15 + Section 16.3 + Section 16.4.\\n\\n\"\n",
        "\n",
        "    \"Very important rules:\\n\"\n",
        "    \"- If the dataset does NOT explicitly state identified/potential/important, do NOT infer it.\\n\"\n",
        "    \"- Do NOT perform causality assessment.\\n\"\n",
        "    \"- If status wording is unclear, categorize as Ongoing or Refuted only.\\n\"\n",
        "    \"- You MUST strictly restrict your reasoning to the information given in the dataset.\\n\"\n",
        "    \"- Do NOT hallucinate: do NOT invent risks, actions, importance level, severity, \"\n",
        "    \"additional evidence, or clinical interpretations not present in the input.\\n\"\n",
        "    \"- Do NOT add assumptions or extend the summary; use ONLY what is written in the row.\\n\\n\"\n",
        "\n",
        "    \"OUTPUT FORMAT REQUIREMENT (MANDATORY):\\n\"\n",
        "    \"Your response MUST contain exactly two lines of text:\\n\"\n",
        "    \"Mapped Sections: <comma-separated section names>\\n\"\n",
        "    \"Decision Logic: <short explanation referencing ONLY the given dataset and rule table>\\n\\n\"\n",
        "\n",
        "    \"Example expected output:\\n\"\n",
        "    \"Mapped Sections: Section 15\\n\"\n",
        "    \"Decision Logic: Status indicates the signal is still under evaluation. \"\n",
        "    \"According to ICH E2C(R2), ongoing signals are only listed in Section 15.\\n\\n\"\n",
        "\n",
        "    \"JSON OUTPUT FORMAT (in addition to the two-line output):\\n\"\n",
        "    \"{\\n\"\n",
        "    \"  \\\"signal\\\": \\\"<signal term>\\\",\\n\"\n",
        "    \"  \\\"scenario\\\": \\\"<one of the seven scenarios>\\\",\\n\"\n",
        "    \"  \\\"mapped_sections\\\": [\\\"Section 15\\\", \\\"Section 16.2\\\", ...],\\n\"\n",
        "    \"  \\\"categorization_summary\\\": \\\"One-sentence classification summary. Must use ONLY dataset information.\\\",\\n\"\n",
        "    \"  \\\"decision_logic\\\": \\\"Full reasoning using rule-based logic; must NOT hallucinate or infer beyond input.\\\"\\n\"\n",
        "    \"}\\n\\n\"\n",
        "\n",
        "    \"Now read the input row and generate BOTH the two-line output AND the JSON output.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3mP5bcECytn",
        "outputId": "bb1b0354-4c63-4429-f62e-1af56bf8b667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   52836.83 ms\n",
            "llama_perf_context_print: prompt eval time =   52835.98 ms /   696 tokens (   75.91 ms per token,    13.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =   95457.85 ms /   145 runs   (  658.33 ms per token,     1.52 tokens per second)\n",
            "llama_perf_context_print:       total time =  148692.46 ms /   841 tokens\n",
            "llama_perf_context_print:    graphs reused =        139\n",
            "Llama.generate: 656 prefix-match hit, remaining 46 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   52836.83 ms\n",
            "llama_perf_context_print: prompt eval time =    3524.64 ms /    46 tokens (   76.62 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =  155616.24 ms /   236 runs   (  659.39 ms per token,     1.52 tokens per second)\n",
            "llama_perf_context_print:       total time =  159809.25 ms /   282 tokens\n",
            "llama_perf_context_print:    graphs reused =        227\n",
            "Llama.generate: 656 prefix-match hit, remaining 53 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   52836.83 ms\n",
            "llama_perf_context_print: prompt eval time =    4007.30 ms /    53 tokens (   75.61 ms per token,    13.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =  133465.26 ms /   202 runs   (  660.72 ms per token,     1.51 tokens per second)\n",
            "llama_perf_context_print:       total time =  138040.29 ms /   255 tokens\n",
            "llama_perf_context_print:    graphs reused =        195\n",
            "Llama.generate: 656 prefix-match hit, remaining 52 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   52836.83 ms\n",
            "llama_perf_context_print: prompt eval time =    3963.93 ms /    52 tokens (   76.23 ms per token,    13.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =  151763.96 ms /   230 runs   (  659.84 ms per token,     1.52 tokens per second)\n",
            "llama_perf_context_print:       total time =  156381.59 ms /   282 tokens\n",
            "llama_perf_context_print:    graphs reused =        222\n",
            "Llama.generate: 656 prefix-match hit, remaining 55 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   52836.83 ms\n",
            "llama_perf_context_print: prompt eval time =    4520.84 ms /    55 tokens (   82.20 ms per token,    12.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =  118353.59 ms /   179 runs   (  661.19 ms per token,     1.51 tokens per second)\n",
            "llama_perf_context_print:       total time =  123373.24 ms /   234 tokens\n",
            "llama_perf_context_print:    graphs reused =        173\n",
            "Llama.generate: 656 prefix-match hit, remaining 52 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   52836.83 ms\n",
            "llama_perf_context_print: prompt eval time =    4984.94 ms /    52 tokens (   95.86 ms per token,    10.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =  199616.11 ms /   299 runs   (  667.61 ms per token,     1.50 tokens per second)\n",
            "llama_perf_context_print:       total time =  205464.49 ms /   351 tokens\n",
            "llama_perf_context_print:    graphs reused =        289\n",
            "Llama.generate: 656 prefix-match hit, remaining 50 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   52836.83 ms\n",
            "llama_perf_context_print: prompt eval time =    3840.88 ms /    50 tokens (   76.82 ms per token,    13.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =  108897.28 ms /   161 runs   (  676.38 ms per token,     1.48 tokens per second)\n",
            "llama_perf_context_print:       total time =  113181.46 ms /   211 tokens\n",
            "llama_perf_context_print:    graphs reused =        155\n",
            "Llama.generate: 656 prefix-match hit, remaining 39 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   52836.83 ms\n",
            "llama_perf_context_print: prompt eval time =    3038.00 ms /    39 tokens (   77.90 ms per token,    12.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =   98777.73 ms /   146 runs   (  676.56 ms per token,     1.48 tokens per second)\n",
            "llama_perf_context_print:       total time =  102215.42 ms /   185 tokens\n",
            "llama_perf_context_print:    graphs reused =        140\n",
            "Llama.generate: 656 prefix-match hit, remaining 46 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   52836.83 ms\n",
            "llama_perf_context_print: prompt eval time =    4628.91 ms /    46 tokens (  100.63 ms per token,     9.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =  115616.50 ms /   171 runs   (  676.12 ms per token,     1.48 tokens per second)\n",
            "llama_perf_context_print:       total time =  120719.57 ms /   217 tokens\n",
            "llama_perf_context_print:    graphs reused =        164\n",
            "Llama.generate: 656 prefix-match hit, remaining 49 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   52836.83 ms\n",
            "llama_perf_context_print: prompt eval time =    3740.24 ms /    49 tokens (   76.33 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =  167094.88 ms /   245 runs   (  682.02 ms per token,     1.47 tokens per second)\n",
            "llama_perf_context_print:       total time =  171532.67 ms /   294 tokens\n",
            "llama_perf_context_print:    graphs reused =        237\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Loop through your signal terms\n",
        "results = []\n",
        "for _, row in df.iterrows():\n",
        "    signal_info = (\n",
        "        f\"Signal: {row['Signal term']}\\n\"\n",
        "        f\"Date detected: {row['Date detected']}\\n\"\n",
        "        f\"Status: {row['Status']}\\n\"\n",
        "        f\"Date closed: {row['Date closed']}\\n\"\n",
        "        f\"Source of signal: {row['Source of signal']}\\n\"\n",
        "        f\"Summary: {row['Summary']}\\n\"\n",
        "        f\"Action: {row['Action']}\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"signal_info: {signal_info}\"}\n",
        "    ]\n",
        "    start_time = time.time()\n",
        "    response = llm.create_chat_completion(\n",
        "        messages=messages,\n",
        "        max_tokens=300,\n",
        "        temperature=0.4\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    gen_time = round(end_time - start_time, 2)\n",
        "    usage = response.get(\"usage\", {})\n",
        "    total_tokens = usage.get(\"total_tokens\", None)\n",
        "    tokens_per_sec = round(total_tokens / gen_time, 2)\n",
        "    # Extract model reply\n",
        "    reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    row_result = row.to_dict()\n",
        "    row_result[\"Mapping\"] = reply\n",
        "    row_result[\"Gen Time\"] = gen_time\n",
        "    row_result[\"Tokens Per Sec\"] = tokens_per_sec\n",
        "    results.append(row_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lqtQNp3YTHhd",
        "outputId": "07e4f942-f021-4478-a6c2-ada0d6ba0623"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b6ae5d23-7715-4e68-8c81-7b0c8edcec7c\", \"mapped_signals_gemma.xlsx\", 7243)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "# Save results\n",
        "mapped_df = pd.DataFrame(results)\n",
        "mapped_df.to_excel(\"mapped_signals_gemma.xlsx\", index=False)\n",
        "files.download(\"mapped_signals_gemma.xlsx\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4248a72d602f472c82de00a0713cd2c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_3a6f79ba35e543f98c04d3ef7abae456"
          }
        },
        "cf2623cb5b194ca79d1c38240c45730b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a7768b778e24c78871e4ce10c34d8ee",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4a2be3cfc9b14a3a9303d7591bc2a14f",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "83d1f71ca5ff415cb6f5d10dfbb9849e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c8af8f6d8adf4118a101667633e53c2a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3fe4f3ac082c4465b4553d036df9d65d",
            "value": ""
          }
        },
        "37c30623fb5a47559bdf98322212a10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_6852f56b1dea4e94b2642011f160aa1e",
            "style": "IPY_MODEL_9f80f5bee44e4b119faf1af4751f672d",
            "value": true
          }
        },
        "897942a248204ba1b5e367c5a0ec554f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_afda049be6b54038a56c4f5ec80c219d",
            "style": "IPY_MODEL_4b7ee0e1d36443388429bb451dd27315",
            "tooltip": ""
          }
        },
        "f3694105712340c19324c32473eba8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e99d569b9d034e20a916ead54af5d111",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d0e9900f465649bbaebb14ba195f108c",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "3a6f79ba35e543f98c04d3ef7abae456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "5a7768b778e24c78871e4ce10c34d8ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a2be3cfc9b14a3a9303d7591bc2a14f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8af8f6d8adf4118a101667633e53c2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fe4f3ac082c4465b4553d036df9d65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6852f56b1dea4e94b2642011f160aa1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f80f5bee44e4b119faf1af4751f672d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afda049be6b54038a56c4f5ec80c219d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b7ee0e1d36443388429bb451dd27315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e99d569b9d034e20a916ead54af5d111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0e9900f465649bbaebb14ba195f108c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cf18c71d015404b902431e9fb8f829f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5289fc786365486791ee1bb0897a678c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_54b4aa6716b74cb693ebe29d817695ac",
            "value": "Connecting..."
          }
        },
        "5289fc786365486791ee1bb0897a678c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54b4aa6716b74cb693ebe29d817695ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d5b607265764a2a979f1ba9beadf3c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d285cd704b141608b7054a0c651d4ea",
              "IPY_MODEL_48209765da0842248e1a087ddea7d7bd",
              "IPY_MODEL_f5f5e7133e564d2da7c9a29c4bad50bf"
            ],
            "layout": "IPY_MODEL_cdcdc9ebf26b4b7689579cf6bd355bce"
          }
        },
        "2d285cd704b141608b7054a0c651d4ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a2e256d2ecd4bd0bd106975a958e2d5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_88f6b93f8fc24e97b7c92ad1344b0cc8",
            "value": "./gemma-7b-it.gguf:‚Äá100%"
          }
        },
        "48209765da0842248e1a087ddea7d7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4785113e0e3b4334a0732a1cfe68502c",
            "max": 34158344288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49fe9956ea784c89ac8e6cf3fedbe3f3",
            "value": 34158344288
          }
        },
        "f5f5e7133e564d2da7c9a29c4bad50bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f59396ed2db64d0c98e5705197d1f35e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_af7046445f3040b3874f073444262151",
            "value": "‚Äá34.2G/34.2G‚Äá[01:33&lt;00:00,‚Äá381MB/s]"
          }
        },
        "cdcdc9ebf26b4b7689579cf6bd355bce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a2e256d2ecd4bd0bd106975a958e2d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88f6b93f8fc24e97b7c92ad1344b0cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4785113e0e3b4334a0732a1cfe68502c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49fe9956ea784c89ac8e6cf3fedbe3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f59396ed2db64d0c98e5705197d1f35e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af7046445f3040b3874f073444262151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}